# -*- coding: utf-8 -*-
"""Simple Model - News Machine

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gIqCSe7y2QQtO-rEANKUnR2TXWIKPMDM
"""

import pandas as pd
import re
import string
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

import seaborn as sns
import matplotlib.pyplot as plt

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout

real = pd.read_csv("True.csv")
fake = pd.read_csv("Fake.csv")

print(real.shape[0])
print(fake.shape[0])

real["label"] = 0
fake["label"] = 1

df = pd.concat([real, fake], ignore_index=True)


# DATA PREPROCESSING
def clean_text(text):
    text = str(text).lower()
    text = re.sub(r"http\S+", "", text)   # remove URLs
    text = text.translate(str.maketrans(string.punctuation, " " * len(string.punctuation)))
    text = re.sub(r"\s+", " ", text).strip()
    return text

df["full_text"] = (df["title"].astype(str) + " " + df["text"].astype(str)).apply(clean_text)

# TRAIN / TEST SPLIT
X_train, X_test, y_train, y_test = train_test_split(
    df["full_text"],
    df["label"],
    test_size=0.2,
    random_state=42,
    stratify=df["label"]
)

# TOKENIZATION
MAX_WORDS = 20000
MAX_LEN = 200

tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token="<UNK>")
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN)
X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN)

# NN MODEL
model = Sequential()

model.add(Embedding(input_dim=MAX_WORDS, output_dim=128, input_length=MAX_LEN))
model.add(GlobalAveragePooling1D())

model.add(Dropout(0.1))
model.add(Dense(64, activation="relu"))

model.add(Dropout(0.1))
model.add(Dense(1, activation="sigmoid"))

model.compile(
    optimizer="adam",
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

# TRAIN MODEL
model.fit(
    X_train_pad, y_train,
    epochs=5,
    batch_size=32,
    validation_split=0.2
)


# EVALUATE MODEL
loss, acc = model.evaluate(X_test_pad, y_test)
print("Test Accuracy:", acc)
print("Test Loss:", loss)

y_pred_prob = model.predict(X_test_pad)
y_pred = (y_pred_prob > 0.5).astype(int).reshape(-1)

# CONFUSION MATRIX
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(6,5))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=["Pred Real", "Pred Fake"],
    yticklabels=["Actual Real", "Actual Fake"]
)
plt.title("Confusion Matrix")
plt.show()

# METRICS
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("\nTrue Negatives:", cm[0][0])
print("False Positives:", cm[0][1])
print("False Negatives:", cm[1][0])
print("True Positives:", cm[1][1])

print("\nAccuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)